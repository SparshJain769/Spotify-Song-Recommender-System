{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8920612,
          "sourceType": "datasetVersion",
          "datasetId": 5365269
        },
        {
          "sourceId": 8920634,
          "sourceType": "datasetVersion",
          "datasetId": 5365283
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-07-10T17:01:08.914944Z",
          "iopub.execute_input": "2024-07-10T17:01:08.915651Z",
          "iopub.status.idle": "2024-07-10T17:01:09.393609Z",
          "shell.execute_reply.started": "2024-07-10T17:01:08.915615Z",
          "shell.execute_reply": "2024-07-10T17:01:09.392561Z"
        },
        "trusted": true,
        "id": "NGkVd5aHo35_",
        "outputId": "57e24a49-d752-4c32-a07b-49d5ba646d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/data-of-25k-songs-ordered/69.csv\n/kaggle/input/selected-users/selected_users.json\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "file_path = '/kaggle/input/data-of-25k-songs-ordered/69.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Load the JSON file\n",
        "json_file_path = '/kaggle/input/selected-users/selected_users.json'\n",
        "with open(json_file_path, 'r') as f:\n",
        "    user_data = json.load(f)\n",
        "\n",
        "\n",
        "track_popularity = data['track_popularity']\n",
        "min_value = data['track_popularity'].min()\n",
        "max_value = data['track_popularity'].max()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-10T17:01:11.995588Z",
          "iopub.execute_input": "2024-07-10T17:01:11.996871Z",
          "iopub.status.idle": "2024-07-10T17:01:43.255831Z",
          "shell.execute_reply.started": "2024-07-10T17:01:11.996825Z",
          "shell.execute_reply": "2024-07-10T17:01:43.254812Z"
        },
        "trusted": true,
        "id": "87QDfYClo36P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the similarity function using these extracted values\n",
        "def absolute_difference_similarity(value1, value2, min_val, max_val):\n",
        "\n",
        "    max_possible_difference = max_val - min_val\n",
        "    difference = abs(value1 - value2)\n",
        "    similarity = 1 - (difference / max_possible_difference)\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def sentiment_similarity_combined(vector1, vector2, alpha_sentiment=0.5):\n",
        "\n",
        "    cos_sim = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "    euc_dist = euclidean_distances(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "    max_dist = np.sqrt(len(vector1))\n",
        "    euc_sim = 1 - (euc_dist / max_dist)\n",
        "    combined_sim = alpha_sentiment * cos_sim + (1 - alpha_sentiment) * euc_sim\n",
        "    return combined_sim\n",
        "\n",
        "def lyrics_embedding_similarity(vector1, vector2):\n",
        "    return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "\n",
        "\n",
        "def combined_audio_similarity(vector1, vector2, alpha_audio=0.5):\n",
        "\n",
        "    cos_sim = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "    euc_dist = euclidean_distances(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "    max_dist = np.sqrt(len(vector1))\n",
        "    euc_sim = 1 - (euc_dist / max_dist)\n",
        "    combined_sim = alpha_audio * cos_sim + (1 - alpha_audio) * euc_sim\n",
        "    return combined_sim\n",
        "\n",
        "def lyrics_language_similarity(lang1, lang2):\n",
        "    return 1 if lang1 == lang2 else 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-10T17:01:47.208892Z",
          "iopub.execute_input": "2024-07-10T17:01:47.209330Z",
          "iopub.status.idle": "2024-07-10T17:01:47.222342Z",
          "shell.execute_reply.started": "2024-07-10T17:01:47.209295Z",
          "shell.execute_reply": "2024-07-10T17:01:47.221162Z"
        },
        "trusted": true,
        "id": "A3nplRYAo36R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(data, index1, index2, weight_popularity=0.2, weight_audio=0.4, weight_language=0.1, weight_sentiment=0.2, weight_lyrics_embedding=0.1, alpha_audio=0.5, alpha_sentiment=0.5):\n",
        "    song1 = data.iloc[index1]\n",
        "    song2 = data.iloc[index2]\n",
        "\n",
        "    popularity_sim = absolute_difference_similarity(song1['track_popularity'], song2['track_popularity'],min_val=min_value,max_val=max_value)\n",
        "\n",
        "    audio_features = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
        "    vector1 = song1[audio_features].values\n",
        "    vector2 = song2[audio_features].values\n",
        "\n",
        "    audio_sim = combined_audio_similarity(vector1.astype(float), vector2.astype(float), alpha_audio)\n",
        "\n",
        "    language_sim = lyrics_language_similarity(song1['lyrics_language'], song2['lyrics_language'])\n",
        "\n",
        "    sentiment_features = ['lyrics_joy', 'lyrics_sadness', 'lyrics_anger', 'lyrics_fear', 'lyrics_surprise', 'lyrics_disgust']\n",
        "    sentiment_vector1 = song1[sentiment_features].values\n",
        "    sentiment_vector2 = song2[sentiment_features].values\n",
        "    sentiment_sim = sentiment_similarity_combined(sentiment_vector1.astype(float), sentiment_vector2.astype(float), alpha_sentiment)\n",
        "\n",
        "    lyrics_embedding_features = [f'lyrics_lyric_embed_{i}' for i in range(768)]\n",
        "    lyrics_embedding_vector1 = song1[lyrics_embedding_features].values\n",
        "    lyrics_embedding_vector2 = song2[lyrics_embedding_features].values\n",
        "    lyrics_embedding_sim = lyrics_embedding_similarity(lyrics_embedding_vector1.astype(float), lyrics_embedding_vector2.astype(float))\n",
        "\n",
        "    overall_similarity = (weight_popularity * popularity_sim) + (weight_audio * audio_sim) + (weight_language * language_sim) + (weight_sentiment * sentiment_sim) + (weight_lyrics_embedding * lyrics_embedding_sim)\n",
        "    return overall_similarity, popularity_sim, audio_sim, language_sim, sentiment_sim, lyrics_embedding_sim"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-10T17:01:51.695756Z",
          "iopub.execute_input": "2024-07-10T17:01:51.696185Z",
          "iopub.status.idle": "2024-07-10T17:01:51.710494Z",
          "shell.execute_reply.started": "2024-07-10T17:01:51.696151Z",
          "shell.execute_reply": "2024-07-10T17:01:51.709260Z"
        },
        "trusted": true,
        "id": "dteuFOD3o36U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_playlist(playlist, data, threshold, weight_popularity, weight_audio, weight_language, weight_sentiment, weight_lyrics_embedding, alpha_audio, alpha_sentiment):\n",
        "    last_song_index = data[data['song_id'] == playlist[-1]].index\n",
        "    if last_song_index.empty:\n",
        "        print(f\"Skipping playlist because the last song with ID {playlist[-1]} was not found in the data.\")\n",
        "        return None, None  # Skip this playlist if the last song is not found\n",
        "\n",
        "    last_song_index = last_song_index[0]\n",
        "    similarities = []\n",
        "\n",
        "    for song_id in playlist[:-1]:\n",
        "        song_index = data[data['song_id'] == song_id].index\n",
        "        if song_index.empty:\n",
        "            continue  # Skip this song if it is not found\n",
        "        song_index = song_index[0]\n",
        "        overall_similarity, _, _, _, _, _ = calculate_similarity(\n",
        "            data, last_song_index, song_index, weight_popularity, weight_audio, weight_language, weight_sentiment, weight_lyrics_embedding, alpha_audio, alpha_sentiment\n",
        "        )\n",
        "        similarities.append(overall_similarity)\n",
        "\n",
        "    if not similarities:\n",
        "        return None, None  # Return None if no valid comparisons were made\n",
        "\n",
        "    avg_similarity = np.mean(similarities)\n",
        "    accuracy = 1 if avg_similarity >= threshold else 0\n",
        "\n",
        "    return avg_similarity, accuracy\n",
        "\n",
        "\n",
        "\n",
        "## Define hyperparameter ranges\n",
        "weight_popularity_values = [0.080]\n",
        "weight_lyrics_embedding_values = [0.005]\n",
        "weight_audio_values = [0.005]\n",
        "weight_sentiment_values = [0.90]\n",
        "weight_language_values = [0.010]\n",
        "\n",
        "alpha_audio_values = [1]\n",
        "alpha_sentiment_values = [0]\n",
        "\n",
        "# Calculate the total number of possible combinations\n",
        "total_combinations = 0\n",
        "for weight_popularity in weight_popularity_values:\n",
        "    for weight_lyrics_embedding in weight_lyrics_embedding_values:\n",
        "        for weight_audio in weight_audio_values:\n",
        "            for weight_sentiment in weight_sentiment_values:\n",
        "                for weight_language in weight_language_values:\n",
        "                    if weight_popularity + weight_lyrics_embedding + weight_audio + weight_sentiment + weight_language == 1:\n",
        "                        for alpha_audio in alpha_audio_values:\n",
        "                            for alpha_sentiment in alpha_sentiment_values:\n",
        "                                total_combinations += 1\n",
        "\n",
        "print(f\"Total number of valid hyperparameter combinations: {total_combinations}\")\n",
        "\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "final_results_df = pd.DataFrame(columns=[\n",
        "    'weight_popularity', 'weight_audio', 'weight_language',\n",
        "    'weight_sentiment', 'weight_lyrics_embedding',\n",
        "    'alpha_audio', 'alpha_sentiment',\n",
        "    'final_avg_similarity_score', 'final_overall_accuracy'\n",
        "])\n",
        "\n",
        "# Analyze each user's playlist for each combination of hyperparameters\n",
        "results = []\n",
        "\n",
        "combination_counter = 0\n",
        "for weight_popularity in weight_popularity_values:\n",
        "    for weight_lyrics_embedding in weight_lyrics_embedding_values:\n",
        "        for weight_audio in weight_audio_values:\n",
        "            for weight_sentiment in weight_sentiment_values:\n",
        "                for weight_language in weight_language_values:\n",
        "                    if weight_popularity + weight_lyrics_embedding + weight_audio + weight_sentiment + weight_language == 1:\n",
        "                        for alpha_audio in alpha_audio_values:\n",
        "                            for alpha_sentiment in alpha_sentiment_values:\n",
        "                                combination_counter += 1\n",
        "                                temp_results = []\n",
        "                                print(f\"Combination {combination_counter}/{total_combinations}: \"\n",
        "                                      f\"weight_popularity={weight_popularity}, weight_audio={weight_audio}, \"\n",
        "                                      f\"weight_language={weight_language}, weight_sentiment={weight_sentiment}, \"\n",
        "                                      f\"weight_lyrics_embedding={weight_lyrics_embedding}, alpha_audio={alpha_audio}, \"\n",
        "                                      f\"alpha_sentiment={alpha_sentiment}\")\n",
        "                                for user in tqdm(user_data, desc=f\"Processing playlists for combination {combination_counter}/{total_combinations}\", leave=False):\n",
        "                                    user_id = user['UserID']\n",
        "                                    playlist = user['SongIDs']\n",
        "\n",
        "                                    avg_similarity, accuracy = evaluate_playlist(playlist, data, 0.7, weight_popularity, weight_audio, weight_language, weight_sentiment, weight_lyrics_embedding, alpha_audio, alpha_sentiment\n",
        ")\n",
        "                                    if avg_similarity is not None and accuracy is not None:\n",
        "                                        temp_results.append({\n",
        "                                            'UserID': user_id,\n",
        "                                            'AverageSimilarityScore': avg_similarity,\n",
        "                                            'Accuracy': accuracy\n",
        "                                        })\n",
        "\n",
        "                                if temp_results:\n",
        "                                    temp_df = pd.DataFrame(temp_results)\n",
        "                                    final_avg_similarity_score = temp_df['AverageSimilarityScore'].mean()\n",
        "                                    final_overall_accuracy = temp_df['Accuracy'].sum() / len(temp_df)\n",
        "\n",
        "                                    # Create a DataFrame for the current combination result\n",
        "                                    current_result_df = pd.DataFrame([{\n",
        "                                        'weight_popularity': weight_popularity,\n",
        "                                        'weight_audio': weight_audio,\n",
        "                                        'weight_language': weight_language,\n",
        "                                        'weight_sentiment': weight_sentiment,\n",
        "                                        'weight_lyrics_embedding': weight_lyrics_embedding,\n",
        "                                        'alpha_audio': alpha_audio,\n",
        "                                        'alpha_sentiment': alpha_sentiment,\n",
        "                                        'final_avg_similarity_score': final_avg_similarity_score,\n",
        "                                        'final_overall_accuracy': final_overall_accuracy\n",
        "                                    }])\n",
        "\n",
        "                                    # Append the current result DataFrame to the final results DataFrame\n",
        "                                    final_results_df = pd.concat([final_results_df, current_result_df], ignore_index=True)\n",
        "\n",
        "                                    # Optionally save the DataFrame to a file after each iteration to ensure progress is not lost\n",
        "                                    final_results_df.to_csv('intermediate_results69.csv', index=False)\n",
        "\n",
        "# Print the final results\n",
        "print(final_results_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-10T10:45:05.365788Z",
          "iopub.execute_input": "2024-07-10T10:45:05.366266Z",
          "iopub.status.idle": "2024-07-10T11:01:36.474639Z",
          "shell.execute_reply.started": "2024-07-10T10:45:05.366230Z",
          "shell.execute_reply": "2024-07-10T11:01:36.473509Z"
        },
        "trusted": true,
        "id": "wY-RjcKRo36X",
        "outputId": "2d218fd1-57af-44f4-fe05-599c5385d308"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Total number of valid hyperparameter combinations: 1\nCombination 1/1: weight_popularity=0.08, weight_audio=0.005, weight_language=0.01, weight_sentiment=0.9, weight_lyrics_embedding=0.005, alpha_audio=1, alpha_sentiment=0\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                                              ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "   weight_popularity  weight_audio  weight_language  weight_sentiment  \\\n0               0.08         0.005             0.01               0.9   \n\n   weight_lyrics_embedding alpha_audio alpha_sentiment  \\\n0                    0.005           1               0   \n\n   final_avg_similarity_score  final_overall_accuracy  \n0                    0.736603                0.681538  \n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_33/554471569.py:114: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  final_results_df = pd.concat([final_results_df, current_result_df], ignore_index=True)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given three songs , handpicked by us , the following functions recommends a song with highest custom similarity score defined above"
      ],
      "metadata": {
        "id": "zntfV6Iso36Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_similar_song(data, index1, index2, index3, weight_popularity=0.025, weight_audio=0.005, weight_language=0.015, weight_sentiment=0.95, weight_lyrics_embedding=0.005, alpha_audio=1, alpha_sentiment=0):\n",
        "    similarities = []\n",
        "\n",
        "    # Get song names for the given indices\n",
        "    song_name1 = data.iloc[index1]['song_title']\n",
        "    song_name2 = data.iloc[index2]['song_title']\n",
        "    song_name3 = data.iloc[index3]['song_title']\n",
        "\n",
        "    # Calculate the similarity of each song in the dataset to the three given songs\n",
        "    for i in tqdm(range(len(data)), desc=\"Calculating similarities\"):\n",
        "        if i in [index1, index2, index3]:\n",
        "            continue  # Skip the given songs themselves\n",
        "\n",
        "        sim1, _, _, _, _, _ = calculate_similarity(data, i, index1, weight_popularity, weight_audio, weight_language, weight_sentiment, weight_lyrics_embedding, alpha_audio, alpha_sentiment)\n",
        "        sim2, _, _, _, _, _ = calculate_similarity(data, i, index2, weight_popularity, weight_audio, weight_language, weight_sentiment, weight_lyrics_embedding, alpha_audio, alpha_sentiment)\n",
        "        sim3, _, _, _, _, _ = calculate_similarity(data, i, index3, weight_popularity, weight_audio, weight_language, weight_sentiment, weight_lyrics_embedding, alpha_audio, alpha_sentiment)\n",
        "\n",
        "        overall_similarity = (sim1 + sim2 + sim3) / 3\n",
        "        similarities.append((i, overall_similarity))\n",
        "\n",
        "    # Sort the similarities in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Find the most similar song that does not have the same name as the original songs\n",
        "    for most_similar_index, _ in similarities:\n",
        "        most_similar_song = data.iloc[most_similar_index]\n",
        "        most_similar_song_name = most_similar_song['song_title']\n",
        "\n",
        "        if most_similar_song_name not in [song_name1, song_name2, song_name3]:\n",
        "            most_similar_song_id = most_similar_song['song_id']\n",
        "            most_similar_artist_name = most_similar_song['artist_name']\n",
        "            return (most_similar_index, most_similar_song_id, most_similar_song_name, most_similar_artist_name,\n",
        "                    song_name1, song_name2, song_name3)\n",
        "\n",
        "    # If no suitable match is found, return None\n",
        "    return None, None, None, None, song_name1, song_name2, song_name3\n",
        "\n",
        "# Example usage\n",
        "index1 = 12456  # Example index 1\n",
        "index2 = 16984  # Example index 2\n",
        "index3 = 15879 # Example index 3\n",
        "\n",
        "(most_similar_index, most_similar_song_id, most_similar_song_name, most_similar_artist_name,\n",
        " song_name1, song_name2, song_name3) = find_most_similar_song(data, index1, index2, index3)\n",
        "\n",
        "if most_similar_index is not None:\n",
        "    print(f\"The most similar song to the songs '{song_name1}', '{song_name2}', and '{song_name3}' is:\\n\")\n",
        "    print(f\"Index: {most_similar_index}\\nSong ID: {most_similar_song_id}\\nSong Name: {most_similar_song_name}\\nArtist Name: {most_similar_artist_name}\")\n",
        "else:\n",
        "    print(f\"No suitable match found for the songs '{song_name1}', '{song_name2}', and '{song_name3}'.\")"
      ],
      "metadata": {
        "id": "poVNaIJfo36f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}